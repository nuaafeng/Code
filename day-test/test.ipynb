{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    # 也可以判断是否为conv2d，使用相应的初始化方式 \n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "     # 是否为批归一化层\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络模型\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc8 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x)) \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "data = torch.tensor(np.load(r\"data/points_results.npy\")).float().to(device)\n",
    "data = data[~torch.isnan(data).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5000, -4.3301,  4.5915,  ...,  0.4791,  0.0000,  0.8777],\n",
      "        [ 2.5091, -4.3249,  4.5913,  ...,  0.4811,  0.0000,  0.8766],\n",
      "        [ 2.5181, -4.3196,  4.5910,  ...,  0.4831,  0.0000,  0.8755],\n",
      "        ...,\n",
      "        [ 2.6438, -4.2439,  4.5877,  ...,  0.5109,  0.0000,  0.8596],\n",
      "        [ 2.6527, -4.2383,  4.5875,  ...,  0.5128,  0.0000,  0.8585],\n",
      "        [ 2.6616, -4.2327,  4.5873,  ...,  0.5148,  0.0000,  0.8573]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data = data[0:10000,:]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 3.6508\n",
      "Epoch [2/10000], Loss: 2.7472\n",
      "Epoch [3/10000], Loss: 3.1072\n",
      "Epoch [4/10000], Loss: 1.8168\n",
      "Epoch [5/10000], Loss: 2.2040\n",
      "Epoch [6/10000], Loss: 2.2043\n",
      "Epoch [7/10000], Loss: 1.7075\n",
      "Epoch [8/10000], Loss: 1.7036\n",
      "Epoch [9/10000], Loss: 0.9314\n",
      "Epoch [10/10000], Loss: 1.0024\n",
      "Epoch [11/10000], Loss: 1.0340\n",
      "Epoch [12/10000], Loss: 0.8749\n",
      "Epoch [13/10000], Loss: 0.6600\n",
      "Epoch [14/10000], Loss: 0.6272\n",
      "Epoch [15/10000], Loss: 0.6683\n",
      "Epoch [16/10000], Loss: 0.4914\n",
      "Epoch [17/10000], Loss: 0.3599\n",
      "Epoch [18/10000], Loss: 0.3589\n",
      "Epoch [19/10000], Loss: 0.2372\n",
      "Epoch [20/10000], Loss: 0.2036\n",
      "Epoch [21/10000], Loss: 0.1666\n",
      "Epoch [22/10000], Loss: 0.1483\n",
      "Epoch [23/10000], Loss: 0.1033\n",
      "Epoch [24/10000], Loss: 0.0954\n",
      "Epoch [25/10000], Loss: 0.0822\n",
      "Epoch [26/10000], Loss: 0.0722\n",
      "Epoch [27/10000], Loss: 0.0802\n",
      "Epoch [28/10000], Loss: 0.0629\n",
      "Epoch [29/10000], Loss: 0.0488\n",
      "Epoch [30/10000], Loss: 0.0500\n",
      "Epoch [31/10000], Loss: 0.0430\n",
      "Epoch [32/10000], Loss: 0.0454\n",
      "Epoch [33/10000], Loss: 0.0512\n",
      "Epoch [34/10000], Loss: 0.0385\n",
      "Epoch [35/10000], Loss: 0.0251\n",
      "Epoch [36/10000], Loss: 0.0333\n",
      "Epoch [37/10000], Loss: 0.0341\n",
      "Epoch [38/10000], Loss: 0.0288\n",
      "Epoch [39/10000], Loss: 0.0399\n",
      "Epoch [40/10000], Loss: 0.0318\n",
      "Epoch [41/10000], Loss: 0.0347\n",
      "Epoch [42/10000], Loss: 0.0512\n",
      "Epoch [43/10000], Loss: 0.0345\n",
      "Epoch [44/10000], Loss: 0.0240\n",
      "Epoch [45/10000], Loss: 0.0470\n",
      "Epoch [46/10000], Loss: 0.0496\n",
      "Epoch [47/10000], Loss: 0.0390\n",
      "Epoch [48/10000], Loss: 0.0466\n",
      "Epoch [49/10000], Loss: 0.0465\n",
      "Epoch [50/10000], Loss: 0.0406\n",
      "Epoch [51/10000], Loss: 0.0394\n",
      "Epoch [52/10000], Loss: 0.0237\n",
      "Epoch [53/10000], Loss: 0.0376\n",
      "Epoch [54/10000], Loss: 0.0385\n",
      "Epoch [55/10000], Loss: 0.0300\n",
      "Epoch [56/10000], Loss: 0.0467\n",
      "Epoch [57/10000], Loss: 0.0298\n",
      "Epoch [58/10000], Loss: 0.0344\n",
      "Epoch [59/10000], Loss: 0.0710\n",
      "Epoch [60/10000], Loss: 0.0439\n",
      "Epoch [61/10000], Loss: 0.0335\n",
      "Epoch [62/10000], Loss: 0.0332\n",
      "Epoch [63/10000], Loss: 0.0485\n",
      "Epoch [64/10000], Loss: 0.0455\n",
      "Epoch [65/10000], Loss: 0.0607\n",
      "Epoch [66/10000], Loss: 0.0282\n",
      "Epoch [67/10000], Loss: 0.0392\n",
      "Epoch [68/10000], Loss: 0.0271\n",
      "Epoch [69/10000], Loss: 0.0314\n",
      "Epoch [70/10000], Loss: 0.0406\n",
      "Epoch [71/10000], Loss: 0.0446\n",
      "Epoch [72/10000], Loss: 0.0349\n",
      "Epoch [73/10000], Loss: 0.0450\n",
      "Epoch [74/10000], Loss: 0.0451\n",
      "Epoch [75/10000], Loss: 0.0360\n",
      "Epoch [76/10000], Loss: 0.0478\n",
      "Epoch [77/10000], Loss: 0.0455\n",
      "Epoch [78/10000], Loss: 0.0379\n",
      "Epoch [79/10000], Loss: 0.0353\n",
      "Epoch [80/10000], Loss: 0.0308\n",
      "Epoch [81/10000], Loss: 0.0336\n",
      "Epoch [82/10000], Loss: 0.0344\n",
      "Epoch [83/10000], Loss: 0.0431\n",
      "Epoch [84/10000], Loss: 0.0477\n",
      "Epoch [85/10000], Loss: 0.0445\n",
      "Epoch [86/10000], Loss: 0.0597\n",
      "Epoch [87/10000], Loss: 0.0371\n",
      "Epoch [88/10000], Loss: 0.0336\n",
      "Epoch [89/10000], Loss: 0.0362\n",
      "Epoch [90/10000], Loss: 0.0368\n",
      "Epoch [91/10000], Loss: 0.0387\n",
      "Epoch [92/10000], Loss: 0.0375\n",
      "Epoch [93/10000], Loss: 0.0467\n",
      "Epoch [94/10000], Loss: 0.0489\n",
      "Epoch [95/10000], Loss: 0.0471\n",
      "Epoch [96/10000], Loss: 0.0442\n",
      "Epoch [97/10000], Loss: 0.0463\n",
      "Epoch [98/10000], Loss: 0.0373\n",
      "Epoch [99/10000], Loss: 0.0261\n",
      "Epoch [100/10000], Loss: 0.0408\n",
      "Epoch [101/10000], Loss: 0.0484\n",
      "Epoch [102/10000], Loss: 0.0385\n",
      "Epoch [103/10000], Loss: 0.0486\n",
      "Epoch [104/10000], Loss: 0.0297\n",
      "Epoch [105/10000], Loss: 0.0534\n",
      "Epoch [106/10000], Loss: 0.0472\n",
      "Epoch [107/10000], Loss: 0.0411\n",
      "Epoch [108/10000], Loss: 0.0408\n",
      "Epoch [109/10000], Loss: 0.0502\n",
      "Epoch [110/10000], Loss: 0.0356\n",
      "Epoch [111/10000], Loss: 0.0345\n",
      "Epoch [112/10000], Loss: 0.0448\n",
      "Epoch [113/10000], Loss: 0.0556\n",
      "Epoch [114/10000], Loss: 0.0369\n",
      "Epoch [115/10000], Loss: 0.0527\n",
      "Epoch [116/10000], Loss: 0.0331\n",
      "Epoch [117/10000], Loss: 0.0375\n",
      "Epoch [118/10000], Loss: 0.0297\n",
      "Epoch [119/10000], Loss: 0.0584\n",
      "Epoch [120/10000], Loss: 0.0369\n",
      "Epoch [121/10000], Loss: 0.0266\n",
      "Epoch [122/10000], Loss: 0.0660\n",
      "Epoch [123/10000], Loss: 0.0647\n",
      "Epoch [124/10000], Loss: 0.0415\n",
      "Epoch [125/10000], Loss: 0.0319\n",
      "Epoch [126/10000], Loss: 0.0487\n",
      "Epoch [127/10000], Loss: 0.0608\n",
      "Epoch [128/10000], Loss: 0.0462\n",
      "Epoch [129/10000], Loss: 0.0564\n",
      "Epoch [130/10000], Loss: 0.0364\n",
      "Epoch [131/10000], Loss: 0.0416\n",
      "Epoch [132/10000], Loss: 0.0347\n",
      "Epoch [133/10000], Loss: 0.0529\n",
      "Epoch [134/10000], Loss: 0.0382\n",
      "Epoch [135/10000], Loss: 0.0416\n",
      "Epoch [136/10000], Loss: 0.0476\n",
      "Epoch [137/10000], Loss: 0.0543\n",
      "Epoch [138/10000], Loss: 0.0378\n",
      "Epoch [139/10000], Loss: 0.0524\n",
      "Epoch [140/10000], Loss: 0.0427\n",
      "Epoch [141/10000], Loss: 0.0270\n",
      "Epoch [142/10000], Loss: 0.0213\n",
      "Epoch [143/10000], Loss: 0.0378\n",
      "Epoch [144/10000], Loss: 0.0688\n",
      "Epoch [145/10000], Loss: 0.0406\n",
      "Epoch [146/10000], Loss: 0.0378\n",
      "Epoch [147/10000], Loss: 0.0358\n",
      "Epoch [148/10000], Loss: 0.0321\n",
      "Epoch [149/10000], Loss: 0.0434\n",
      "Epoch [150/10000], Loss: 0.0409\n",
      "Epoch [151/10000], Loss: 0.0228\n",
      "Epoch [152/10000], Loss: 0.0377\n",
      "Epoch [153/10000], Loss: 0.0240\n",
      "Epoch [154/10000], Loss: 0.0321\n",
      "Epoch [155/10000], Loss: 0.0479\n",
      "Epoch [156/10000], Loss: 0.0523\n",
      "Epoch [157/10000], Loss: 0.0304\n",
      "Epoch [158/10000], Loss: 0.0397\n",
      "Epoch [159/10000], Loss: 0.0350\n",
      "Epoch [160/10000], Loss: 0.0233\n",
      "Epoch [161/10000], Loss: 0.0204\n",
      "Epoch [162/10000], Loss: 0.0263\n",
      "Epoch [163/10000], Loss: 0.0385\n",
      "Epoch [164/10000], Loss: 0.0443\n",
      "Epoch [165/10000], Loss: 0.0479\n",
      "Epoch [166/10000], Loss: 0.0392\n",
      "Epoch [167/10000], Loss: 0.0350\n",
      "Epoch [168/10000], Loss: 0.0282\n",
      "Epoch [169/10000], Loss: 0.0286\n",
      "Epoch [170/10000], Loss: 0.0590\n",
      "Epoch [171/10000], Loss: 0.0633\n",
      "Epoch [172/10000], Loss: 0.0271\n",
      "Epoch [173/10000], Loss: 0.0514\n",
      "Epoch [174/10000], Loss: 0.0426\n",
      "Epoch [175/10000], Loss: 0.0270\n",
      "Epoch [176/10000], Loss: 0.0410\n",
      "Epoch [177/10000], Loss: 0.0537\n",
      "Epoch [178/10000], Loss: 0.0390\n",
      "Epoch [179/10000], Loss: 0.0521\n",
      "Epoch [180/10000], Loss: 0.0402\n",
      "Epoch [181/10000], Loss: 0.0416\n",
      "Epoch [182/10000], Loss: 0.0470\n",
      "Epoch [183/10000], Loss: 0.0477\n",
      "Epoch [184/10000], Loss: 0.0615\n",
      "Epoch [185/10000], Loss: 0.0439\n",
      "Epoch [186/10000], Loss: 0.0320\n",
      "Epoch [187/10000], Loss: 0.0411\n",
      "Epoch [188/10000], Loss: 0.0542\n",
      "Epoch [189/10000], Loss: 0.0325\n",
      "Epoch [190/10000], Loss: 0.0400\n",
      "Epoch [191/10000], Loss: 0.0368\n",
      "Epoch [192/10000], Loss: 0.0504\n",
      "Epoch [193/10000], Loss: 0.0289\n",
      "Epoch [194/10000], Loss: 0.0415\n",
      "Epoch [195/10000], Loss: 0.0499\n",
      "Epoch [196/10000], Loss: 0.0427\n",
      "Epoch [197/10000], Loss: 0.0281\n",
      "Epoch [198/10000], Loss: 0.0542\n",
      "Epoch [199/10000], Loss: 0.0452\n",
      "Epoch [200/10000], Loss: 0.0297\n",
      "Epoch [201/10000], Loss: 0.0435\n",
      "Epoch [202/10000], Loss: 0.0432\n",
      "Epoch [203/10000], Loss: 0.0549\n",
      "Epoch [204/10000], Loss: 0.0462\n",
      "Epoch [205/10000], Loss: 0.0329\n",
      "Epoch [206/10000], Loss: 0.0521\n",
      "Epoch [207/10000], Loss: 0.0403\n",
      "Epoch [208/10000], Loss: 0.0610\n",
      "Epoch [209/10000], Loss: 0.0378\n",
      "Epoch [210/10000], Loss: 0.0446\n",
      "Epoch [211/10000], Loss: 0.0310\n",
      "Epoch [212/10000], Loss: 0.0422\n",
      "Epoch [213/10000], Loss: 0.0324\n",
      "Epoch [214/10000], Loss: 0.0516\n",
      "Epoch [215/10000], Loss: 0.0331\n",
      "Epoch [216/10000], Loss: 0.0446\n",
      "Epoch [217/10000], Loss: 0.0250\n",
      "Epoch [218/10000], Loss: 0.0327\n",
      "Epoch [219/10000], Loss: 0.0513\n",
      "Epoch [220/10000], Loss: 0.0301\n",
      "Epoch [221/10000], Loss: 0.0198\n",
      "Epoch [222/10000], Loss: 0.0408\n",
      "Epoch [223/10000], Loss: 0.0256\n",
      "Epoch [224/10000], Loss: 0.0394\n",
      "Epoch [225/10000], Loss: 0.0250\n",
      "Epoch [226/10000], Loss: 0.0579\n",
      "Epoch [227/10000], Loss: 0.0625\n",
      "Epoch [228/10000], Loss: 0.0362\n",
      "Epoch [229/10000], Loss: 0.0440\n",
      "Epoch [230/10000], Loss: 0.0506\n",
      "Epoch [231/10000], Loss: 0.0368\n",
      "Epoch [232/10000], Loss: 0.0555\n",
      "Epoch [233/10000], Loss: 0.0504\n",
      "Epoch [234/10000], Loss: 0.0289\n",
      "Epoch [235/10000], Loss: 0.0506\n",
      "Epoch [236/10000], Loss: 0.0553\n",
      "Epoch [237/10000], Loss: 0.0302\n",
      "Epoch [238/10000], Loss: 0.0279\n",
      "Epoch [239/10000], Loss: 0.0365\n",
      "Epoch [240/10000], Loss: 0.0483\n",
      "Epoch [241/10000], Loss: 0.0461\n",
      "Epoch [242/10000], Loss: 0.0365\n",
      "Epoch [243/10000], Loss: 0.0239\n",
      "Epoch [244/10000], Loss: 0.0371\n",
      "Epoch [245/10000], Loss: 0.0393\n",
      "Epoch [246/10000], Loss: 0.0493\n",
      "Epoch [247/10000], Loss: 0.0380\n",
      "Epoch [248/10000], Loss: 0.0274\n",
      "Epoch [249/10000], Loss: 0.0156\n",
      "Epoch [250/10000], Loss: 0.0419\n",
      "Epoch [251/10000], Loss: 0.0480\n",
      "Epoch [252/10000], Loss: 0.0206\n",
      "Epoch [253/10000], Loss: 0.0333\n",
      "Epoch [254/10000], Loss: 0.0265\n",
      "Epoch [255/10000], Loss: 0.0402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 30\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m'\u001b[39m,loss,epoch)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:384\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m workspace\n\u001b[1;32m    382\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[0;32m--> 384\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    385\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/torch/utils/tensorboard/summary.py:333\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m make_np(tensor)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    336\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/torch/utils/tensorboard/_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _prepare_pytorch(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/torch/utils/tensorboard/_convert_np.py:30\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_pytorch\u001b[39m(x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(input_size, hidden_size, output_size).to(device)\n",
    "model.apply(weight_init)\n",
    "#torch中的apply函数通过可以不断遍历model的各个模块，并将weight_init函数应用在这些Module上\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.9)\n",
    "\n",
    "# 假设你已经有一个名为data的数据集，包含1000000个样本\n",
    "# data应该是一个Tensor，每行是一个样本，前两列是输入，后四列是输出\n",
    "# 使用DataLoader来加载数据集\n",
    "# DataLoader的batch_size可以根据你的实际情况进行调整\n",
    "input = data[:,]\n",
    "data_loader = DataLoader(TensorDataset(data[:, :2], data[:, 3:]), batch_size=32, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10000\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "        # 前向传播\n",
    "        # Q1 output 不是单位向量\n",
    "        # Q2 1e6个点太多了，所以nan\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('Loss/train',loss,epoch)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'dune_model/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
